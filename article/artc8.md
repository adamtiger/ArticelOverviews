# Article Summary

**Title:** How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments <br/>
**Authors:** CÃ©dric Colas, Olivier Sigaud and Pierre-Yves Oudeyer <br/>
**Year:** 2018

### Citation:

@ARTICLE{
2018arXiv180608295C, </br>
author = {{Colas}, C. and {Sigaud}, O. and {Oudeyer}, P.-Y.}, </br>
title = "{How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments}", </br>
journal = {ArXiv e-prints}, </br>
archivePrefix = "arXiv", </br>
eprint = {1806.08295}, </br>
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}, </br>
year = 2018, </br>
month = jun, </br>
adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180608295C}, </br>
adsnote = {Provided by the SAO/NASA Astrophysics Data System} </br>
}

### Abstract:

Consistently checking the statistical significance of experimental results
is one of the mandatory methodological steps to address the so-called 
"reproducibility crisis" in deep reinforcement learning. In this tutorial paper,
we explain how the number of random seeds relates to the probabilities of
statistical errors. For both the t-test and the bootstrap confidence interval
test, we recall theoretical guidelines to determine the number of random
seeds one should use to provide a statistically significant comparison of
the performance of two algorithms. Finally, we discuss the influence of
deviations from the assumptions usually made by statistical tests. We
show that they can lead to inaccurate evaluations of statistical errors and
provide guidelines to counter these negative effects. We make our code
available to perform the tests.

## Overview

Here a structured but concise overview on the article is provided.

### Technical background


### Results, experiments


### Articles referred

